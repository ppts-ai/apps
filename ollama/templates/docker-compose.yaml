services:
  ollama:
    image: ollama/ollama:latest  # Replace with the exact Ollama + Llama 3.2 image name if different
    container_name: ollama
    devices:
      - nvidia.com/gpu=all
    ports:
      - "11434:11434"  # Expose the port if the service needs to be accessed externally
    volumes:
      - ${MODELS}/ollama/models:/root/.ollama/models
    environment:
      - OLLAMA_ORIGINS="*"

  ui:
    image: icexpr/ollama-ui:1.9  # Replace with the exact Ollama + Llama 3.2 image name if different
    container_name: ui
    ports:
      - "8000:8000"  # Expose the port if the service needs to be accessed externally